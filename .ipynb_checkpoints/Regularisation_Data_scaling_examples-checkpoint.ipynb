{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c291a85c",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Examples: Regularisation - Data scaling\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this notebook, we'll gain a comprehensive understanding of scaling and how to implement it in your models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "- Understand what scaling and standardisation are.\n",
    "- Understand the code required to implement data scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce595e8",
   "metadata": {},
   "source": [
    "## Introducing Scaling\n",
    "\n",
    "Scaling data is crucial when preparing it for machine learning models, especially for those that involve regularisation. Regularisation techniques, such as L1 (Lasso) and L2 (Ridge) regularisation, adjust model complexity by applying penalties to the coefficients of predictors. The magnitude of these penalties is influenced by the scale of the predictors, making scaling an essential step to ensure fairness and effectiveness in regularisation. Essentially, if the features are on different scales, the model might unfairly penalise smaller scale features more than those on a larger scale. Therefore, to apply regularisation uniformly across all features, we must standardise their scales.\n",
    "\n",
    "There are two common scaling techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bad04",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "One way to do this is with $[0,1]$-normalisation, otherwise known as min-max normalisation: squeezing your data into the range $[0,1]$. Through normalisation, the maximum value of a variable becomes one, the minimum becomes zero, and the values in-between become decimals between zero and one.\n",
    "\n",
    "We implement this transformation by applying the following operation to each of the values of a predictor variable:\n",
    "\n",
    "$$\\hat{x}_{ij} = \\frac{x_{ij}-min(x_j)}{max(x_j)-min(x_j)},$$\n",
    "\n",
    "where $\\hat{x}_{ij}$ is the value after normalisation, $x_{ij}$ is the $i^{th}$ item of $x_j$, and $min()$, $max()$ return the smallest and largest values of variable $x_j$ respectively. \n",
    "\n",
    "Normalisation is useful because it ensures all variables share the same range: $[0,1]$. One problem with normalisation, however, is that if there are outliers, the bulk of your data will all lie in a small range, so you would lose information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dba616",
   "metadata": {},
   "source": [
    "### Standardisation\n",
    "\n",
    "Z-score standardisation, or simply standardisation, on the other hand, does not suffer from this drawback as it handles outliers gracefully. \n",
    "\n",
    "We implement Z-score standardisation by applying the following operation to each of our variables: \n",
    "\n",
    "$$\\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}.$$\n",
    "\n",
    "Here, $\\mu_j$ represents the mean of variable $x_j$, while $\\sigma_j$ is the variable's standard deviation. As can be seen from the above formula, instead of dividing by the full range of our variable, we instead divide by a more distribution-aware measure in the standard deviation. While this doesn't completely remove the effects of outliers, it does consider them in a more conservative manner. As a trade-off to using this transformation, our variable is no longer contained within the $[0,1]$ range as it was during normalisation (in fact, it can now take on a range which includes negative values). This means that all our variables won't be bound to the exact same range (i.e. they can have slightly different influence levels on the learnt regression coefficients during regularisation), but they are far closer to one another then they were before the use of standardisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b41ef",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To begin, let's import a few Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eea416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd052b",
   "metadata": {},
   "source": [
    "Now we'll load our data as a Pandas DataFrame after fetching it from the GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33df9b5",
   "metadata": {},
   "source": [
    "We can take a look at the dimensions of the dataframe to get an idea of the number of rows, _n_, and number of predictors, _p_, which is equal to one less than the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac19046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506733d",
   "metadata": {},
   "source": [
    "Our dataset contains various environmental indicators related to SDG 15, such as deforestation rates, protected area coverage, biodiversity indices, and other relevant variables. Our objective is to model an environmental outcome for  the health of biodiversity using these indicators. \n",
    "\n",
    "The mathematical representation of our model can be described as follows:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n",
    "\n",
    "In this formulation, $Y$ represents the response variable, which, in our case, is `BiodiversityHealthIndex`. This response variable is influenced by _p_ predictor variables ($X_1, X_2, ..., X_p$), each representing different environmental indicators relevant to SDG 15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a40164",
   "metadata": {},
   "source": [
    "We can see in the data above that the variables have different scales. For example, variables such as `ConservationFunding` may involve financial values potentially reaching into high numerical ranges, whereas other variables like `ProtectedAreaCoverage` or `RenewableEnergyUsage` are expressed as percentages. So let's go ahead and implement scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e50774",
   "metadata": {},
   "source": [
    "## Implementing Scaling\n",
    "\n",
    "Let's see how we standardise the features. Sklearn makes rescaling easy. We'll import the `StandardScaler()` object from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d494fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into predictors and response\n",
    "X = df.drop('BiodiversityHealthIndex', axis=1)\n",
    "y = df['BiodiversityHealthIndex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scaler method from sklearn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaler object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fb6d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the scaled predictor values into a dataframe\n",
    "X_standardise = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "X_standardise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01942ade",
   "metadata": {},
   "source": [
    "Taking a look at one of the variables as an example (`SoilQualityIndex`), we can see that standardising the data has caused it to be centered around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62341cd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(X_standardise['SoilQualityIndex'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71ead2",
   "metadata": {},
   "source": [
    "Furthermore, the standard error within each variable in the data is now equal to one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardise.describe().loc['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b6070",
   "metadata": {},
   "source": [
    "## Implementing min-max normalisation\n",
    "\n",
    "Let's see how we normalise the features. Sklearn makes rescaling easy. We'll import the `MinMaxScaler()` object from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scaler method from sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ab718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaler object\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee017bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749fc7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the scaled predictor values into a dataframe\n",
    "X_standardise = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "X_standardise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a5a28",
   "metadata": {},
   "source": [
    "Taking a look at one of the variables as an example (`SoilQualityIndex`), we can see that normalising the data put it neatly between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3279492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(X_standardise['SoilQualityIndex'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770cf4e5",
   "metadata": {},
   "source": [
    "Furthermore, the standard error for these newly normalised variables is all relatively similar at +-0.28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f885d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardise.describe().loc['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8682a0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we have seen or been introduced to:\n",
    "\n",
    "- The difference between scaling and standardising the predictor variables in our dataset\n",
    "- The different scaling techniques and performed scaling on our data using Standardisation and Normalisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130eb39",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Links to additional resources to help with the understanding of concepts presented in the train.\n",
    "\n",
    "- [Article on standard min-max normalization vs z-score standardisation](https://www.codecademy.com/articles/normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
